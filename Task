** 1. Import Required Libraries **

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

** 2. Load the Titanic Dataset **

titanic_data = pd.read_csv("titanic.csv")  # Change to your file path if needed
print("Preview of the dataset:")
print(titanic_data.head())

** 3. Quick Data Overview **

print("\nBasic dataset info:")
print(titanic_data.info())

print("\nMissing values in each column:")
print(titanic_data.isnull().sum())

print(f"\nOverall survival rate: {titanic_data['Survived'].mean()*100:.2f}%")

** 4. Simple Data Visualizations **

plt.figure(figsize=(5,4))
sns.countplot(data=titanic_data, x='Survived', palette="Set2")
plt.title("Survival Distribution")
plt.show()

plt.figure(figsize=(5,4))
sns.countplot(data=titanic_data, x='Pclass', hue='Survived', palette="coolwarm")
plt.title("Survival by Passenger Class")
plt.show()

plt.figure(figsize=(5,4))
sns.histplot(data=titanic_data, x='Age', hue='Survived', multiple="stack", bins=20)
plt.title("Age vs Survival")
plt.show()

** 5. Data Cleaning **

titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)
titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)

# Dropping unnecessary columns
titanic_data.drop(columns=['Cabin', 'Ticket', 'Name', 'PassengerId'], inplace=True)

** 6. Encoding Categorical Features **

encoder = LabelEncoder()
titanic_data['Sex'] = encoder.fit_transform(titanic_data['Sex'])
titanic_data['Embarked'] = encoder.fit_transform(titanic_data['Embarked'])

** 7. Feature Engineering **

titanic_data['FamilySize'] = titanic_data['SibSp'] + titanic_data['Parch'] + 1
titanic_data['IsAlone'] = (titanic_data['FamilySize'] == 1).astype(int)

titanic_data.drop(columns=['SibSp', 'Parch'], inplace=True)

print("\nSample after cleaning and feature engineering:")
print(titanic_data.head())

** 8. Splitting the Dataset **

X = titanic_data.drop(columns=['Survived'])
y = titanic_data['Survived']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

** 9. Model-1: Decision Tree **

tree_model = DecisionTreeClassifier(random_state=42)
tree_model.fit(X_train, y_train)
tree_preds = tree_model.predict(X_test)

tree_acc = accuracy_score(y_test, tree_preds)
print(f"\nDecision Tree Accuracy: {tree_acc:.2f}")

** 10. Model-2: Random Forest **

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

rf_acc = accuracy_score(y_test, rf_preds)
print(f"Random Forest Accuracy: {rf_acc:.2f}")

** 11. Model-3: Logistic Regression **

log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train, y_train)
log_preds = log_model.predict(X_test)

log_acc = accuracy_score(y_test, log_preds)
print(f"Logistic Regression Accuracy: {log_acc:.2f}")

** 12. Confusion Matrix for Random Forest **

cm = confusion_matrix(y_test, rf_preds)
plt.figure(figsize=(4,3))
sns.heatmap(cm, annot=True, fmt="d", cmap="Purples")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Random Forest")
plt.show()

** 13. Final Conclusion **

print("\n--- Summary ---")
print(f"Decision Tree Accuracy: {tree_acc*100:.2f}%")
print(f"Random Forest Accuracy: {rf_acc*100:.2f}%")
print(f"Logistics Regression Accuracy:{log_acc*100:.2f}%")
print("\nInsights:")
print("Random Forest achieved the highest accuracy in this test split.")
Print("Gender, Passenger class, And whealither a passenger was alone seem to be individual.")
print("Adding features like 'IsAlone' improveed model performance slightly.")
print("Future work could include tuning the boosting algorithm for better results.")
